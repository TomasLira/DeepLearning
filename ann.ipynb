{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95792f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import annotations\n",
    "from typing import Callable, Optional, Literal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "165421da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test  = np.hstack([np.ones((X_test.shape[0], 1)), X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation = Callable[[np.ndarray], np.ndarray]\n",
    "LossFn     = Callable[[np.ndarray, np.ndarray], float]\n",
    "\n",
    "\n",
    "class ANN:\n",
    "    \"\"\"\n",
    "    Build a feedforward net with flexible sizing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        Number of input features.\n",
    "    output_size : int\n",
    "        Number of outputs.\n",
    "    net_length : int\n",
    "        Number of hidden layers.\n",
    "    net_width : int | list[int]\n",
    "        - If int: every hidden layer has this many neurons (requires `net_length`).\n",
    "        - If list[int]: each entry is the width of that hidden layer (net_length inferred).\n",
    "    hidden_activation : Activation | Literal[\"relu\",\"sigmoid\",\"tanh\",\"leaky_relu\"]\n",
    "        Activation used on hidden layers (callable or known string).\n",
    "    output_activation : Activation | Literal[\"identity\",\"sigmoid\",\"tanh\",\"softmax\"]\n",
    "        Activation used on the output layer (callable or known string).\n",
    "    loss : LossFn | Literal[\"mse\",\"bce\",\"cross_entropy\"]\n",
    "        Loss function (callable or known string).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        net_length: int,\n",
    "        net_width: int | list[int],\n",
    "        hidden_activation: Activation | Literal[\"relu\",\"sigmoid\",\"tanh\",\"leaky_relu\"],\n",
    "        output_activation: Activation | Literal[\"identity\",\"sigmoid\",\"tanh\",\"softmax\"],\n",
    "        loss: LossFn | Literal[\"mse\",\"bce\",\"cross_entropy\"],\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        # initial configs\n",
    "        self.hidden_activation = self._resolve_activation(hidden_activation)\n",
    "        self.output_activation = self._resolve_activation(output_activation)\n",
    "        self.loss_fn           = self._resolve_loss(loss)\n",
    "        self.seed              = seed\n",
    "        self.input_size        = input_size\n",
    "        self.output_size       = output_size\n",
    "\n",
    "        self.net_length = net_length\n",
    "        self.net_width  = net_width if isinstance(net_width, list) else [net_width] * net_length\n",
    "\n",
    "    def _resolve_activation(self, fn):\n",
    "        if callable(fn):\n",
    "            return fn\n",
    "\n",
    "        lut = {\n",
    "            \"relu\":        lambda x: np.maximum(0, x),\n",
    "            \"sigmoid\":     lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
    "            \"tanh\":        np.tanh,\n",
    "            \"leaky_relu\":  lambda x, a=0.01: np.where(x > 0, x, a * x),\n",
    "            \"identity\":    lambda x: x,\n",
    "            \"softmax\":     lambda x: (\n",
    "                (lambda z: (np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)))(\n",
    "                    x - np.max(x, axis=1, keepdims=True)\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "        if isinstance(fn, str) and fn in lut:\n",
    "            return lut[fn]\n",
    "        raise ValueError(f\"Unknown activation: {fn!r}\")\n",
    "\n",
    "    def _resolve_loss(self, fn):\n",
    "        if callable(fn):\n",
    "            return fn\n",
    "\n",
    "        def _mse(y, t):\n",
    "            return np.mean((y - t) ** 2)\n",
    "\n",
    "        def _bce(y, t, eps=1e-12):\n",
    "            y = np.clip(y, eps, 1 - eps)\n",
    "            return -np.mean(t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "\n",
    "        def _cross_entropy(y, t, eps=1e-12):\n",
    "            y = np.clip(y, eps, 1 - eps)\n",
    "            # allow class indices or one-hot targets\n",
    "            if t.ndim == 1 or (t.ndim == 2 and t.shape[1] == 1):\n",
    "                t = t.reshape(-1)\n",
    "                return -np.mean(np.log(y[np.arange(y.shape[0]), t]))\n",
    "            return -np.mean(np.sum(t * np.log(y), axis=1))\n",
    "\n",
    "        lut = {\n",
    "            \"mse\": _mse,\n",
    "            \"bce\": _bce,\n",
    "            \"cross_entropy\": _cross_entropy,\n",
    "        }\n",
    "        if isinstance(fn, str) and fn in lut:\n",
    "            return lut[fn]\n",
    "        raise ValueError(f\"Unknown loss: {fn!r}\")\n",
    "\n",
    "    def _initialize_weights(self, X):\n",
    "        self.weights = []\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # First layer: input to first hidden\n",
    "        self.weights.append(\n",
    "            np.random.uniform(-1, 1, size=(X.shape[1], self.net_width[0]))\n",
    "        )\n",
    "\n",
    "        # Hidden layers\n",
    "        for layer_pos in range(1, self.net_length):\n",
    "            in_size = self.net_width[layer_pos - 1]\n",
    "            out_size = self.net_width[layer_pos]\n",
    "            self.weights.append(\n",
    "                np.random.uniform(-1, 1, size=(in_size, out_size))\n",
    "            )\n",
    "\n",
    "        # Output layer: last hidden to output\n",
    "        self.weights.append(\n",
    "            np.random.uniform(-1, 1, size=(self.net_width[-1], self.output_size))\n",
    "        )\n",
    "\n",
    "    def _forward_pass(self, X: np.ndarray) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "        if not hasattr(self, \"weights\") or not self.weights:\n",
    "            self._initialize_weights(X)\n",
    "\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # hidden layers\n",
    "        for W in self.weights[:-1]:\n",
    "            z = activations[-1] @ W\n",
    "            pre_activations.append(z)\n",
    "            a = self.hidden_activation(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # output layer\n",
    "        z = activations[-1] @ self.weights[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = self.output_activation(z)\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b67003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X: (4, 3) yhat: (4, 3) num layers (weights): 3\n",
      "Row sums (softmax): [1. 1. 1. 1.]\n",
      "Loss: 0.9847953461577265\n"
     ]
    }
   ],
   "source": [
    "# forward pass with softmax + cross-entropy (class indices)\n",
    "X = np.random.randn(4, 3)\n",
    "y_idx = np.array([0, 2, 0, 2])\n",
    "\n",
    "net = ANN(\n",
    "    input_size=3,\n",
    "    output_size=3,\n",
    "    net_length=2,\n",
    "    net_width=[5, 4],\n",
    "    hidden_activation=\"relu\",\n",
    "    output_activation=\"softmax\",\n",
    "    loss=\"cross_entropy\",\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "acts, pre = net._forward_pass(X)\n",
    "yhat = acts[-1]\n",
    "\n",
    "print(\"Shapes ->\",\n",
    "      \"X:\", X.shape,\n",
    "      \"yhat:\", yhat.shape,\n",
    "      \"num layers (weights):\", len(net.weights))\n",
    "\n",
    "print(\"Row sums (softmax):\", np.round(yhat.sum(axis=1), 6))\n",
    "print(\"Loss:\", net.loss_fn(yhat, y_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
